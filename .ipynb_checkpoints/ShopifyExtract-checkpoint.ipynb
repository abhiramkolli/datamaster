{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "import platform\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import tz\n",
    "import dateutil.parser as dp\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendates(shoptimezone, min_date,max_date,rfreq):\n",
    "    to_zone = tz.gettz(shoptimezone)\n",
    "    dateranges = pd.date_range(start=min_date, end=max_date, freq=rfreq, tz=to_zone)\n",
    "    dateranges = dateranges.union([min_date,max_date])\n",
    "    dfdateranges = pd.DataFrame(dateranges)\n",
    "    dfdateranges.columns=['start_date']\n",
    "    dfdateranges['end_date'] = dfdateranges.start_date.shift(-1)\n",
    "    dfdateranges = dfdateranges[:-1]\n",
    "    dfdateranges['end_date'] = dfdateranges['end_date'] + datetime.timedelta(milliseconds=1)\n",
    "    return dfdateranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcurtimeinshoptz(sz):\n",
    "    from_zone = tz.tzlocal()\n",
    "    to_zone = tz.gettz(sz)\n",
    "    utc = datetime.datetime.now()\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    currentshopdate = utc.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconvtimeinshoptz(sz, t):\n",
    "    to_zone = tz.gettz(sz)\n",
    "    currentshopdate = t.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcountandpages(countrurl, headers, cntparams):\n",
    "    totalcnt = requests.get(countrurl, headers = headers, params = cntparams).json()['count']   \n",
    "    nopages = math.ceil(totalcnt/limit) + 1\n",
    "    return totalcnt,nopages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettimezone(shoptimeurl, headers, cntparams):\n",
    "    response = requests.get(shoptimeurl, headers = headers, params = cntparams).json()\n",
    "    df = pd.DataFrame(response['shop'], index=[0])\n",
    "    return df['iana_timezone'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfirstcreationdate(shopifycode, pageurl, headers, params):\n",
    "    params.update({'order' : 'created_at asc'})\n",
    "    response = requests.get(pageurl, headers = headers, params = params).json()\n",
    "    df = pd.DataFrame(response[shopifycode])\n",
    "    min_date = min(df['created_at'])\n",
    "    print(min_date)\n",
    "    return min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshopifydates(rtype, rfreq, min_date, max_date, shopifycode, shoptimezone, countrurl, headers, cntparams, pageurl, params):\n",
    "    if (rtype == runtype[0]) or (rtype == runtype[1] and min_date is None and max_date is None):\n",
    "        currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "        to_zone = tz.gettz(shoptimezone)\n",
    "        min_date_str = getfirstcreationdate(shopifycode, pageurl, headers, params)\n",
    "        min_date = dp.parse(min_date_str)\n",
    "        min_date = min_date.replace(tzinfo=to_zone)\n",
    "        dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "    elif rtype == runtype[1]:\n",
    "        if max_date is None:    \n",
    "            dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "        else:    \n",
    "            dates = gendates(shoptimezone, min_date, max_date, rfreq)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushtojson(dfcontents, dest_file_name):\n",
    "    dfcontents.to_json(dest_file_name,orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestshopifydata(shopifycode, pageurl, params): \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        response = requests_retry_session().get(pageurl, headers = headers, params = params)\n",
    "    except Exception as x:\n",
    "        print('It failed :(', x.__class__.__name__)\n",
    "    else:\n",
    "        print('It eventually worked', response.status_code)\n",
    "    finally:\n",
    "        t1 = time.time()\n",
    "        print('Took', t1 - t0, 'seconds')\n",
    "    #response = requests.get(pageurl, headers = headers, params = params)\n",
    "    df = pd.DataFrame(response.json()[shopifycode])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_details(client_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_details\n",
    "        WHERE client_name = @client_name\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_name', 'STRING', client_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvendor_details(vendor_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.vendor_details\n",
    "        WHERE vendor_name = @vendor_name\n",
    "        ORDER BY vendor_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('vendor_name', 'STRING', vendor_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_shopify_entitilements(client_id):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_shopify_entitilements\n",
    "        WHERE client_id = @client_id\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_id', 'INTEGER', client_id)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlastupdateddate(dataset_name, table_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"select max(updated_at) max_updated_dt from \" + dataset_id + \".\" + table_name + \";\"\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteexistingrows(dataset_name, table_name, ids):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    query = \"delete from \" + dataset_id + \".\" + table_name + \" where id in UNNEST(@ids);\"\n",
    "    query_params = [\n",
    "        bigquery.ArrayQueryParameter('ids', 'INTEGER', ids)\n",
    "    ]\n",
    "\n",
    "    print(query)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.use_legacy_sql = False\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "    \n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createloadtracker(dataset_id,table_name,file_names,date_from,date_to):\n",
    "    load_id = uuid.uuid4()\n",
    "    dfnew = pd.DataFrame(columns=['load_id','dataset_id','table_name','file_names',\n",
    "    'date_from','date_to','loaded_to_bigquery','bigquery_load_date','creation_date',\n",
    "    'update_date','load_script_version','load_script_file_name'])\n",
    "    row = dict()\n",
    "    row['load_id'] = str(load_id)\n",
    "    row['dataset_id'] = dataset_id\n",
    "    row['table_name'] = table_name\n",
    "    row['file_names'] = file_names\n",
    "    row['date_from'] = date_from.replace(tzinfo=None)\n",
    "    row['date_to'] = date_to.replace(tzinfo=None)\n",
    "    row['loaded_to_bigquery'] = False\n",
    "    row['bigquery_load_date'] = None\n",
    "    row['creation_date'] = datetime.datetime.now()\n",
    "    row['update_date'] = datetime.datetime.now()\n",
    "    row['load_script_version'] = 'v1'\n",
    "    row['load_script_file_name'] = 'shopifyextract.py'\n",
    "    #row_s = pd.Series(row)    \n",
    "    #print(row_s)\n",
    "    #dfnew = dfnew.append(row_s,ignore_index=True)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateloadtracker(filename, dataset_id, file_names, table_name, date_from, date_to, delimitertype, loadtype, skipheader):\n",
    "    delimitertype = 'NEWLINE_DELIMITED_CSV'\n",
    "    loadtype = 'WRITE_APPEND'   \n",
    "    loadtracker = createloadtracker(dataset_id,table_name,file_names,date_from,date_to)\n",
    "    #loadtracker.to_csv(filename, index=False)\n",
    "    insertintobigquery(loadtracker, dataset_id, 'load_tracker', delimitertype, loadtype, skipheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocalfiletogooglestorage(batfile, source_file_name, dest_file_name):\n",
    "    pass_arg=[]\n",
    "    pass_arg.append(batfile)\n",
    "    pass_arg.append(source_file_name)\n",
    "    pass_arg.append(dest_file_name)\n",
    "    p = Popen(pass_arg, stdout=PIPE, stderr=PIPE)\n",
    "    output, errors = p.communicate()\n",
    "    p.wait() # wait for process to terminate\n",
    "    print(output)\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertintobigquery(loadtracker, dataset_id, table_name, delimitertype, loadtype, skipheader):\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    table_ref = client.dataset(dataset_id).table(table_name)\n",
    "    print(table_ref)\n",
    "    table = client.get_table(table_ref)  # API Request   \n",
    "    row = tuple([loadtracker[field.name] for field in table.schema])\n",
    "    print(row)\n",
    "    errors = client.insert_rows(table, [row])  # API request\n",
    "    print(errors)    \n",
    "    assert errors == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_path = os.getcwd()\n",
    "gspath = 'gs://sarasmaster'\n",
    "os.chdir(os.getcwd())\n",
    "filesep = '\\\\' if platform.system() == 'Windows' else '/'\n",
    "gssep = '/'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"creds\" + filesep + \"sarasmaster-524142bf5547.json\"\n",
    "gcspath = 'C:\\\\Users\\\\kabhi\\\\AppData\\\\Local\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + gcspath\n",
    "batfile = app_path + filesep + 'movetogcs.bat' if platform.system() == 'Windows' else app_path + filesep + 'movetogcs.sh'\n",
    "delimitertype = 'NEWLINE_DELIMITED_JSON'\n",
    "loadtype = 'WRITE_APPEND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\logs\\shopify_20180509.log\n"
     ]
    }
   ],
   "source": [
    "client_details = getclient_details('Kopari Beauty')\n",
    "client_shopify_entitilements = getclient_shopify_entitilements(client_details.client_id)\n",
    "shopifyurl= client_shopify_entitilements.shop_url\n",
    "cloud_storage_dir = client_shopify_entitilements.cloud_storage_dir\n",
    "access_token = client_shopify_entitilements.access_token\n",
    "project_id = client_details.project_id\n",
    "dataset_id = client_shopify_entitilements.dataset_id\n",
    "pageno = 1\n",
    "limit = 250\n",
    "\n",
    "filepath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'shopify'\n",
    "gcspath = gspath + gssep + client_shopify_entitilements.cloud_storage_dir + gssep + 'shopify'\n",
    "logpath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'logs'\n",
    "loadtrackerfile = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'loadtracker.csv'\n",
    "loadtrackertable = 'load_tracking'\n",
    "hdlr = logging.FileHandler(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "print(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Start Shopify Load Process')\n",
    "\n",
    "status = 'any'\n",
    "runtype = ['full','incremental']\n",
    "headers = {'content-type' : 'application/json', 'X-Shopify-Access-Token' : access_token}\n",
    "urlparams = {'limit': limit, 'status' : status}\n",
    "cnturlparams = {'status' : status}\n",
    "dot = '.'\n",
    "skipheader = None\n",
    "shopurl = shopifyurl + '/admin/shop.json'\n",
    "shopifycodes = {\n",
    "    'shopifycodes': ['orders', 'customers', 'products'],\n",
    "    'pageurl': [shopifyurl + '/admin/orders.json', shopifyurl + '/admin/customers.json', shopifyurl + '/admin/products.json'],\n",
    "    'countrurl': [shopifyurl + '/admin/orders/count.json', shopifyurl + '/admin/customers/count.json', shopifyurl + '/admin/products/count.json'],\n",
    "    'dest_file_name': [filepath + filesep + 'orders' + filesep + 'inbox' + filesep + 'orders', filepath + filesep + 'customers' + filesep + 'inbox' + filesep + 'customers', filepath + filesep + 'products' + filesep + 'inbox' + filesep + 'products'],\n",
    "    'gs_file_path': [gcspath + gssep + 'orders' + gssep + 'inbox', gcspath + gssep + 'customers' + gssep + 'inbox', gcspath + gssep + 'products' + gssep + 'inbox'],\n",
    "    'gs_file_name': [gcspath + gssep + 'orders' + gssep + 'orders', gcspath + gssep + 'customers' + gssep + 'customers', gcspath + gssep + 'products' + gssep + 'products'],\n",
    "    'dest_table_name': ['shopify_orders', 'shopify_customers', 'shopify_products'],\n",
    "    'dest_file_type': ['json', 'json', 'json']\n",
    "}\n",
    "\n",
    "dfshopifycodes = pd.DataFrame(shopifycodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updateloadtracker(loadtrackerfile, dataset_id, 'Test', 'shopify_orders', datetime.datetime.now(), datetime.datetime.now(), None, None, skipheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name:orders\n",
      "Total Count:764\n",
      "Total Pages:5\n",
      "Start Date:20180508201046\n",
      "End Date:20180509201046\n",
      "It eventually worked 200\n",
      "Took 0.39401912689208984 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3669109344482422 seconds\n",
      "It eventually worked 200\n",
      "Took 0.7007293701171875 seconds\n",
      "It eventually worked 200\n",
      "Took 0.32161688804626465 seconds\n",
      "Number of ids to be checked for delete:764\n",
      "Table Name:orders\n",
      "Total Count:3\n",
      "Total Pages:2\n",
      "Start Date:20180509201046\n",
      "End Date:20180509202409\n",
      "It eventually worked 200\n",
      "Took 0.39270806312561035 seconds\n",
      "Number of ids to be checked for delete:3\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180508.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180508.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180508.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  5.5 MiB]                                                \\r-\\r- [0 files][  2.1 MiB/  5.5 MiB]                                                \\r\\\\\\r\\\\ [0 files][  4.4 MiB/  5.5 MiB]                                                \\r|\\r| [1 files][  5.5 MiB/  5.5 MiB]                                                \\r/\\r\\r\\nOperation completed over 1 objects/5.5 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180509.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180509.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180509.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/ 17.6 KiB]                                                \\r/ [1 files][ 17.6 KiB/ 17.6 KiB]                                                \\r\\r\\nOperation completed over 1 objects/17.6 KiB.                                     \\r\\n'\n",
      "TableReference('sarasmaster-201918', 'kopari', 'load_tracker')\n",
      "('d504d39b-6976-492d-91f2-9089839a174e', 'kopari', 'shopify_orders', 'C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180508.json,C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180509.json', datetime.datetime(2018, 5, 8, 20, 10, 46, 1000), datetime.datetime(2018, 5, 9, 20, 24, 9, 747038), False, None, datetime.datetime(2018, 5, 9, 22, 24, 43, 567889), datetime.datetime(2018, 5, 9, 22, 24, 43, 567889), 'v1', 'shopifyextract.py')\n",
      "[]\n",
      "Table Name:customers\n",
      "Total Count:299\n",
      "Total Pages:3\n",
      "Start Date:20180508200137\n",
      "End Date:20180509200137\n",
      "It eventually worked 200\n",
      "Took 3.405853748321533 seconds\n",
      "It eventually worked 200\n",
      "Took 1.576549768447876 seconds\n",
      "Number of ids to be checked for delete:299\n",
      "Table Name:customers\n",
      "Total Count:5\n",
      "Total Pages:2\n",
      "Start Date:20180509200137\n",
      "End Date:20180509202409\n",
      "It eventually worked 200\n",
      "Took 1.2845325469970703 seconds\n",
      "Number of ids to be checked for delete:5\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\customers\\inbox\\customers_20180508.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180508.json gs://sarasmaster/kopari/shopify/customers/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180508.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/443.7 KiB]                                                \\r/ [1 files][443.7 KiB/443.7 KiB]                                                \\r-\\r\\r\\nOperation completed over 1 objects/443.7 KiB.                                    \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\customers\\inbox\\customers_20180509.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180509.json gs://sarasmaster/kopari/shopify/customers/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180509.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  8.3 KiB]                                                \\r/ [1 files][  8.3 KiB/  8.3 KiB]                                                \\r\\r\\nOperation completed over 1 objects/8.3 KiB.                                      \\r\\n'\n",
      "TableReference('sarasmaster-201918', 'kopari', 'load_tracker')\n",
      "('5edffcf9-a0b5-4956-88c7-c3cdb31d8f48', 'kopari', 'shopify_customers', 'C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180508.json,C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\customers\\\\inbox\\\\customers_20180509.json', datetime.datetime(2018, 5, 8, 20, 1, 37, 1000), datetime.datetime(2018, 5, 9, 20, 24, 9, 747038), False, None, datetime.datetime(2018, 5, 9, 22, 25, 14, 687712), datetime.datetime(2018, 5, 9, 22, 25, 14, 687712), 'v1', 'shopifyextract.py')\n",
      "[]\n",
      "Table Name:products\n",
      "Total Count:1\n",
      "Total Pages:2\n",
      "Start Date:20180424091345\n",
      "End Date:20180425091345\n",
      "It eventually worked 200\n",
      "Took 0.39602231979370117 seconds\n",
      "Number of ids to be checked for delete:1\n",
      "Table Name:products\n",
      "Total Count:1\n",
      "Total Pages:2\n",
      "Start Date:20180425091345\n",
      "End Date:20180426091345\n",
      "It eventually worked 200\n",
      "Took 0.1958622932434082 seconds\n",
      "Number of ids to be checked for delete:1\n",
      "Table Name:products\n",
      "Total Count:0\n",
      "Total Pages:1\n",
      "Start Date:20180426091345\n",
      "End Date:20180427091345\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7ea0d22a05ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mgcsfilelist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgcsfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mpushtojson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of ids to be checked for delete:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;31m#deleteexistingrows(dataset_id, row['dest_table_name'], df['id'].tolist())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "shoptimezone = gettimezone(shopurl, headers, cnturlparams)\n",
    "currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "runmode = runtype[1]\n",
    "runfreq = 'D'\n",
    "for row_index,row in dfshopifycodes.iterrows():\n",
    "    lastshopdate = getlastupdateddate(dataset_id, row['dest_table_name']).max_updated_dt\n",
    "    if lastshopdate is not None:\n",
    "        lastshopdate = getconvtimeinshoptz(shoptimezone, lastshopdate)\n",
    "        lastshopdate = lastshopdate + datetime.timedelta(milliseconds=1)\n",
    "        start_date = lastshopdate\n",
    "        end_date = currentshopdate\n",
    "    else:\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "        \n",
    "    dates = getshopifydates(runmode,runfreq,start_date,end_date,row['shopifycodes'], shoptimezone, row['countrurl'], headers, cnturlparams, row['pageurl'], urlparams)\n",
    "    ids = []\n",
    "    localfilelist = []\n",
    "    gcsfilelist = []\n",
    "    for dates_index, dates_row in dates.iterrows():\n",
    "        cntparams = cnturlparams\n",
    "        cntparams.update({'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "        totalcnt,nopages = getcountandpages(row['countrurl'], headers, cntparams)\n",
    "        print(\"Table Name:\" + row['shopifycodes'])\n",
    "        print(\"Total Count:\" + str(totalcnt))\n",
    "        print(\"Total Pages:\" + str(nopages))\n",
    "        print(\"Start Date:\" + dates_row['start_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        print(\"End Date:\" + dates_row['end_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        if totalcnt > 0:\n",
    "            df = pd.DataFrame()\n",
    "            for i in range(1,nopages):\n",
    "                params = urlparams\n",
    "                params.update({'page': i,'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "                df1 = requestshopifydata(row['shopifycodes'], row['pageurl'], params)\n",
    "                df=df.append(df1,ignore_index=True)\n",
    "                time.sleep(1)\n",
    "            ids.extend(df['id'].tolist()) if df.shape[0] > 0 else ids\n",
    "            localfilename = row['dest_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "            gcsfilename = row['gs_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "            localfilelist.append(localfilename)\n",
    "            gcsfilelist.append(gcsfilename)\n",
    "            pushtojson(df, localfilename)\n",
    "            print(\"Number of ids to be checked for delete:\" + str(len(df['id'].tolist())))\n",
    "        #deleteexistingrows(dataset_id, row['dest_table_name'], df['id'].tolist())        \n",
    "\n",
    "    \n",
    "    for localfilename in localfilelist:\n",
    "        print(localfilename)\n",
    "        loadlocalfiletogooglestorage(batfile, localfilename, row['gs_file_path'])\n",
    "    filenames = ','.join(localfilelist)\n",
    "    updateloadtracker(loadtrackerfile, dataset_id, filenames, row['dest_table_name'], start_date, end_date, delimitertype, loadtype, skipheader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datamaster)",
   "language": "python",
   "name": "datamaster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
