{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import platform\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import tz\n",
    "import dateutil.parser as dp\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendates(shoptimezone, min_date,max_date,rfreq):\n",
    "    to_zone = tz.gettz(shoptimezone)\n",
    "    dateranges = pd.date_range(start=min_date, end=max_date, freq=rfreq, tz=to_zone)\n",
    "    dateranges = dateranges.union([min_date,max_date])\n",
    "    dfdateranges = pd.DataFrame(dateranges)\n",
    "    dfdateranges.columns=['start_date']\n",
    "    dfdateranges['end_date'] = dfdateranges.start_date.shift(-1)\n",
    "    dfdateranges = dfdateranges[:-1]\n",
    "    dfdateranges['end_date'] = dfdateranges['end_date'] + datetime.timedelta(milliseconds=1)\n",
    "    return dfdateranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcurtimeinshoptz(sz):\n",
    "    from_zone = tz.tzlocal()\n",
    "    to_zone = tz.gettz(sz)\n",
    "    utc = datetime.datetime.now()\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    currentshopdate = utc.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconvtimeinshoptz(sz, t):\n",
    "    to_zone = tz.gettz(sz)\n",
    "    currentshopdate = t.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcountandpages(countrurl, headers, cntparams):\n",
    "    totalcnt = requests.get(countrurl, headers = headers, params = cntparams).json()['count']   \n",
    "    nopages = math.ceil(totalcnt/limit) + 1\n",
    "    return totalcnt,nopages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettimezone(shoptimeurl, headers, cntparams):\n",
    "    response = requests.get(shoptimeurl, headers = headers, params = cntparams).json()\n",
    "    df = pd.DataFrame(response['shop'], index=[0])\n",
    "    return df['iana_timezone'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfirstcreationdate(shopifycode, pageurl, headers, params):\n",
    "    params.update({'order' : 'created_at asc'})\n",
    "    response = requests.get(pageurl, headers = headers, params = params).json()\n",
    "    df = pd.DataFrame(response[shopifycode])\n",
    "    min_date = min(df['created_at'])\n",
    "    return min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshopifydates(rtype, rfreq, min_date, max_date, shopifycode, shoptimezone, countrurl, headers, cntparams, pageurl, params):\n",
    "    if (rtype == runtype[0]) or (rtype == runtype[1] and min_date == None and max_date == None):\n",
    "        currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "        to_zone = tz.gettz(shoptimezone)\n",
    "        min_date_str = getfirstcreationdate(shopifycode, pageurl, headers, params)\n",
    "        min_date = dp.parse(min_date_str)\n",
    "        min_date = min_date.replace(tzinfo=to_zone)\n",
    "        dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "    elif rtype == runtype[1]:\n",
    "        if max_date == None:    \n",
    "            dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "            return dates\n",
    "        else:    \n",
    "            dates = gendates(shoptimezone, min_date, max_date, rfreq)\n",
    "            return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushtojson(dfcontents, dest_file_name):\n",
    "    dfcontents.to_json(dest_file_name,orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestshopifydata(shopifycode, pageurl, params): \n",
    "    response = requests.get(pageurl, headers = headers, params = params)\n",
    "    df = pd.DataFrame(response.json()[shopifycode])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_details(client_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_details\n",
    "        WHERE client_name = @client_name\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_name', 'STRING', client_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvendor_details(vendor_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.vendor_details\n",
    "        WHERE vendor_name = @vendor_name\n",
    "        ORDER BY vendor_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('vendor_name', 'STRING', vendor_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_shopify_entitilements(client_id):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_shopify_entitilements\n",
    "        WHERE client_id = @client_id\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_id', 'INTEGER', client_id)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlastupdateddate(dataset_name, table_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"select max(updated_at) max_updated_dt from \" + dataset_id + \".\" + table_name + \";\"\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteexistingrows(dataset_name, table_name, ids):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    query = \"delete from \" + dataset_id + \".\" + table_name + \" where id in UNNEST(@ids);\"\n",
    "    query_params = [\n",
    "        bigquery.ArrayQueryParameter('ids', 'INTEGER', ids)\n",
    "    ]\n",
    "\n",
    "    print(query)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.use_legacy_sql = False\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "    \n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateloadtracker(dataset_name, table_name, ids):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    query = \"delete from \" + dataset_id + \".\" + table_name + \" where id in UNNEST(@ids);\"\n",
    "    query_params = [\n",
    "        bigquery.ArrayQueryParameter('ids', 'INTEGER', ids)\n",
    "    ]\n",
    "\n",
    "    print(query)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.use_legacy_sql = False\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "    \n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocalfiletogooglestorage(batfile, source_file_name, dest_file_name):\n",
    "    pass_arg=[]\n",
    "    pass_arg.append(batfile)\n",
    "    pass_arg.append(source_file_name)\n",
    "    pass_arg.append(dest_file_name)\n",
    "    p = Popen(pass_arg, stdout=PIPE, stderr=PIPE)\n",
    "    output, errors = p.communicate()\n",
    "    p.wait() # wait for process to terminate\n",
    "    print(output)\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiletobigquery(file_name, dataset_id, table_name, delimitertype, loadtype, skipheader):\n",
    "    client = bigquery.Client()\n",
    "    table_ref = client.dataset(dataset_id).table(table_name)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    if skipheader is not None:\n",
    "        job_config.skip_leading_rows = skipheader\n",
    "    job_config.source_format = delimitertype\n",
    "    if delimitertype == bigquery.SourceFormat.CSV:\n",
    "        job_config.autodetect = True\n",
    "    job_config.write_disposition = loadtype\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        file_name,\n",
    "        table_ref,\n",
    "        job_config=job_config)  # API request\n",
    "\n",
    "    assert load_job.job_type == 'load'\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "\n",
    "    assert load_job.state == 'DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_path = os.getcwd()\n",
    "gspath = 'gs://sarasmaster'\n",
    "os.chdir(os.getcwd())\n",
    "filesep = '\\\\' if platform.system() == 'Windows' else '/'\n",
    "gssep = '/'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"creds\" + filesep + \"sarasmaster-524142bf5547.json\"\n",
    "gcspath = 'C:\\\\Users\\\\kabhi\\\\AppData\\\\Local\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + gcspath\n",
    "batfile = app_path + filesep + 'movetogcs.bat' if platform.system() == 'Windows' else app_path + filesep + 'movetogcs.sh'\n",
    "delimitertype = 'NEWLINE_DELIMITED_JSON'\n",
    "loadtype = 'WRITE_APPEND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabhi\\Desktop\\datamaster\\kopari\\logs\\shopify_20180429.log\n"
     ]
    }
   ],
   "source": [
    "client_details = getclient_details('Kopari Beauty')\n",
    "client_shopify_entitilements = getclient_shopify_entitilements(client_details.client_id)\n",
    "shopifyurl= client_shopify_entitilements.shop_url\n",
    "cloud_storage_dir = client_shopify_entitilements.cloud_storage_dir\n",
    "access_token = client_shopify_entitilements.access_token\n",
    "project_id = client_details.project_id\n",
    "dataset_id = client_shopify_entitilements.dataset_id\n",
    "pageno = 1\n",
    "limit = 250\n",
    "\n",
    "filepath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'shopify'\n",
    "gcspath = gspath + gssep + client_shopify_entitilements.cloud_storage_dir + gssep + 'shopify'\n",
    "logpath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'logs'\n",
    "hdlr = logging.FileHandler(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "print(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Start Shopify Load Process')\n",
    "\n",
    "status = 'any'\n",
    "runtype = ['full','incremental']\n",
    "headers = {'content-type' : 'application/json', 'X-Shopify-Access-Token' : access_token}\n",
    "urlparams = {'limit': limit, 'status' : status}\n",
    "cnturlparams = {'status' : status}\n",
    "dot = '.'\n",
    "skipheader = None\n",
    "shopurl = shopifyurl + '/admin/shop.json'\n",
    "shopifycodes = {\n",
    "    'shopifycodes': ['orders', 'customers', 'products'],\n",
    "    'pageurl': [shopifyurl + '/admin/orders.json', shopifyurl + '/admin/customers.json', shopifyurl + '/admin/products.json'],\n",
    "    'countrurl': [shopifyurl + '/admin/orders/count.json', shopifyurl + '/admin/customers/count.json', shopifyurl + '/admin/products/count.json'],\n",
    "    'dest_file_name': [filepath + filesep + 'orders' + filesep + 'orders', filepath + filesep + 'customers' + filesep + 'customers', filepath + filesep + 'products' + filesep + 'products'],\n",
    "    'gs_file_path': [gcspath + gssep + 'orders', gcspath + gssep + 'customers', gcspath + gssep + 'products'],\n",
    "    'gs_file_name': [gcspath + gssep + 'orders' + gssep + 'orders', gcspath + gssep + 'customers' + gssep + 'customers', gcspath + gssep + 'products' + gssep + 'products'],\n",
    "    'dest_table_name': ['shopify_orders', 'shopify_customers', 'shopify_products'],\n",
    "    'dest_file_type': ['json', 'json', 'json']\n",
    "}\n",
    "\n",
    "dfshopifycodes = pd.DataFrame(shopifycodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astimezone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c91ce94f42ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfshopifycodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlastshopdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetlastupdateddate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dest_table_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_updated_dt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlastshopdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetconvtimeinshoptz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshoptimezone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastshopdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mlastshopdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlastshopdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmilliseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mstart_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlastshopdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b84209431145>\u001b[0m in \u001b[0;36mgetconvtimeinshoptz\u001b[1;34m(sz, t)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetconvtimeinshoptz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mto_zone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcurrentshopdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastimezone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_zone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcurrentshopdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astimezone'"
     ]
    }
   ],
   "source": [
    "shoptimezone = gettimezone(shopurl, headers, cnturlparams)\n",
    "currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "runmode = runtype[1]\n",
    "runfreq = 'D'\n",
    "for row_index,row in dfshopifycodes.iterrows():\n",
    "    lastshopdate = getlastupdateddate(dataset_id, row['dest_table_name']).max_updated_dt\n",
    "    if lastshopdate is not None:\n",
    "        lastshopdate = getconvtimeinshoptz(shoptimezone, lastshopdate)\n",
    "        lastshopdate = lastshopdate + datetime.timedelta(milliseconds=1)\n",
    "        start_date = lastshopdate\n",
    "        end_date = currentshopdate\n",
    "    else:\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "        \n",
    "    print(lastshopdate)\n",
    "    dates = getshopifydates(runmode,runfreq,start_date,end_date,row['shopifycodes'], shoptimezone, row['countrurl'], headers, cnturlparams, row['pageurl'], urlparams)\n",
    "    ids = []\n",
    "    localfilelist = []\n",
    "    gcsfilelist = []\n",
    "    for dates_index, dates_row in dates.iterrows():\n",
    "        cntparams = cnturlparams\n",
    "        cntparams.update({'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "        totalcnt,nopages = getcountandpages(row['countrurl'], headers, cntparams)\n",
    "        print(\"Table Name:\" + row['shopifycodes'])\n",
    "        print(\"Total Count:\" + str(totalcnt))\n",
    "        print(\"Total Pages:\" + str(nopages))\n",
    "        print(\"Start Date:\" + dates_row['start_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        print(\"End Date:\" + dates_row['end_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(1,nopages):\n",
    "            params = urlparams\n",
    "            params.update({'page': i,'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "            df1 = requestshopifydata(row['shopifycodes'], row['pageurl'], params)\n",
    "            df=df.append(df1,ignore_index=True)\n",
    "            time.sleep(1)\n",
    "        ids.extend(df['id'].tolist()) if df.shape[0] > 0 else ids\n",
    "        localfilename = row['dest_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "        gcsfilename = row['gs_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "        localfilelist.append(localfilename)\n",
    "        gcsfilelist.append(gcsfilename)\n",
    "        pushtojson(df, localfilename)\n",
    "    \n",
    "    print(\"Number of ids to be checked for delete:\" + str(len(ids)))\n",
    "    #deleteexistingrows(dataset_id, row['dest_table_name'], ids)        \n",
    "    for localfilename in localfilelist:\n",
    "        loadlocalfiletogooglestorage(batfile, localfilename, row['gs_file_path'])\n",
    "        #os.remove(localfilename)\n",
    "    for gcsfilename in gcsfilelist:\n",
    "        loadfiletobigquery(gcsfilename, dataset_id, row['dest_table_name'], delimitertype, loadtype, skipheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shopify_orders\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the error stream for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-7295169b0ee2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfshopifycodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dest_table_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloadfiletobigquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gs://sarasmaster/kopari/shopify/orders/orders_20180427.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfshopifycodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dest_table_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimitertype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloadtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-fc84e3bf9a6c>\u001b[0m in \u001b[0;36mloadfiletobigquery\u001b[1;34m(file_name, dataset_id, table_name, delimitertype, loadtype, skipheader)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mload_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'load'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mload_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Waits for table load to complete.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mload_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DONE'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\google\\api_core\\future\\polling.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the error stream for more details."
     ]
    }
   ],
   "source": [
    "print(dfshopifycodes['dest_table_name'][0])\n",
    "loadfiletobigquery('gs://sarasmaster/kopari/shopify/orders/orders_20180427.json', dataset_id, dfshopifycodes['dest_table_name'][0], delimitertype, loadtype, skipheader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
