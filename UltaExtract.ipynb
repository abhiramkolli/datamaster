{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "import platform\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import dateutil.parser as dp\n",
    "import pandas.io.gbq as gbq\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_path = os.getcwd()\n",
    "gspath = 'gs://sarasmaster'\n",
    "os.chdir(os.getcwd())\n",
    "filesep = '\\\\' if platform.system() == 'Windows' else '/'\n",
    "gssep = '/'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"creds\" + filesep + \"sarasmaster-524142bf5547.json\"\n",
    "gcspath = 'C:\\\\Users\\\\kabhi\\\\AppData\\\\Local\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + gcspath\n",
    "batfile = app_path + filesep + 'movetogcs.bat' if platform.system() == 'Windows' else app_path + filesep + 'movetogcs.sh'\n",
    "delimitertype = 'CSV'\n",
    "loadtype = 'WRITE_APPEND'\n",
    "skipheader = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocalfiletogooglestorage(batfile, source_file_name, dest_file_name):\n",
    "    pass_arg=[]\n",
    "    pass_arg.append(batfile)\n",
    "    pass_arg.append(source_file_name)\n",
    "    pass_arg.append(dest_file_name)\n",
    "    p = Popen(pass_arg, stdout=PIPE, stderr=PIPE)\n",
    "    output, errors = p.communicate()\n",
    "    p.wait() # wait for process to terminate\n",
    "    print(output)\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiletobigquery(file_name, dataset_id, table_name, delimitertype, loadtype, skipheader):\n",
    "    client = bigquery.Client()\n",
    "    table_ref = client.dataset(dataset_id).table(table_name)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    if skipheader is not None:\n",
    "        job_config.skip_leading_rows = skipheader\n",
    "    job_config.source_format = delimitertype\n",
    "    #if delimitertype == bigquery.SourceFormat.CSV:\n",
    "        #job_config.autodetect = True\n",
    "    job_config.write_disposition = loadtype\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        file_name,\n",
    "        table_ref,\n",
    "        job_config=job_config)  # API request\n",
    "\n",
    "    assert load_job.job_type == 'load'\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "\n",
    "    assert load_job.state == 'DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addcolumnames(dfdata):\n",
    "    dfdata.columns = ['dummy','vendor_id','vendor_name','item_number','item_description','div_id','dept_id','class_id','sub_class_id',\n",
    "    'item_rank','store_count','upc_no','vendor_part_no','brand_id','brand_name','purch_cost_avg','retail_price_cur','feature_code',\n",
    "    'season_code','total_sales_ty_units','total_sales_ly_units','total_sales_units_chg_per','total_sales_ty','total_sales_ly','total_sales_chg_per',\n",
    "    'comp_sales_ty_units','comp_sales_ly_units','comp_sales_units_chg_per','comp_sales_ty','comp_sales_ly','comp_sales_chg_per',\n",
    "    'noncomp_sales_ty_units','noncomp_sales_ly_units','noncomp_sales_units_chg_per','noncomp_sales_ty','noncomp_sales_ly','noncomp_sales_chg_per',\n",
    "    'dotcom_sales_ty_units','dotcom_sales_ly_units','dotcom_sales_units_chg_per','dotcom_sales_ty','dotcom_sales_ly','dotcom_sales_chg_per',\n",
    "    'totaleoh_sales_ty_units','totaleoh_sales_ly_units','totaleoh_sales_units_chg_per','totaleoh_sales_ty','totaleoh_sales_ly','totaleoh_sales_chg_per',\n",
    "    'totalwos','dceoh_sales_ty_units','dceoh_sales_ly_units','dceoh_sales_units_chg_per','dceoh_sales_ty','dceoh_sales_ly','dceoh_sales_chg_per',\n",
    "    'dc_on_order_units','dc_dotcom_order_units','dc_dotcom_wos','dc_total_wos','storeeoh_sales_ty_units','storeeoh_sales_ly_units','storeeoh_sales_units_chg_per',\n",
    "    'storeeoh_sales_ty','storeeoh_sales_ly','storeeoh_sales_chg_per','storeeoh_comp_units','storeeoh_noncomp_units','store_on_order_units',\n",
    "    'store_in_transit_units','store_avg_sales','store_comp_wos','store_noncomp_wos','store_total_wos']\n",
    "    return dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfiles(filename):\n",
    "    df = pd.read_excel(filename, sheet_name=0,header=None)\n",
    "    dfdata = df[9:]\n",
    "    dfdata = dfdata.reset_index(drop=True)\n",
    "    dfheaders = df[:9]\n",
    "    date_str = dfheaders.iloc[2][3].split(':')[1]\n",
    "    end_date_str = date_str.split('-')[1]\n",
    "    end_date_str = end_date_str.strip()\n",
    "    start_date_str = date_str.split('-')[0]\n",
    "    start_date_str = start_date_str.strip()\n",
    "    start_date = datetime.strptime(start_date_str, '%m/%d/%Y')\n",
    "    end_date = datetime.strptime(end_date_str, '%m/%d/%Y')\n",
    "    dfdata = addcolumnames(dfdata)\n",
    "    no_week = start_date.isocalendar()[1]\n",
    "    currency_cd = 'USD'\n",
    "    quarter = pd.Timestamp(start_date).quarter\n",
    "    year = pd.Timestamp(start_date).year\n",
    "    load_id = uuid.uuid4()\n",
    "    dfdata['load_id'] = load_id\n",
    "    dfdata['start_date'] = start_date\n",
    "    dfdata['end_date'] = end_date\n",
    "    dfdata['year'] = year\n",
    "    dfdata['qtr'] = quarter\n",
    "    dfdata['week'] = no_week\n",
    "    dfdata['currency_code'] = currency_cd\n",
    "    print(start_date)\n",
    "    print(end_date)   \n",
    "    dfdata = dfdata.drop('dummy', axis=1)\n",
    "    localfilename = app_path + filesep + 'kopari' + filesep + 'ulta' + filesep + 'inbox' + filesep + 'ulta_sales_inv_'\n",
    "    gsfilepath = gspath + gssep + 'kopari' + gssep + 'ulta' + gssep + 'inbox'\n",
    "    localfilename = localfilename + start_date.strftime('%Y%m%d') + '_' + end_date.strftime('%Y%m%d') + '.csv'\n",
    "    dfdata.to_csv(localfilename, index=False)\n",
    "    #loadlocalfiletogooglestorage(batfile, localfilename, gsfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales and Inventory Report - 2018 FW02 - KOCO LIFE (49509).xlsx\n",
      "2018-02-11 00:00:00\n",
      "2018-02-17 00:00:00\n",
      "Sales and Inventory Report - 2018 FW03 - KOCO LIFE (49509).xlsx\n",
      "2018-02-18 00:00:00\n",
      "2018-02-24 00:00:00\n",
      "Sales and Inventory Report - 2018 FW06 - KOCO LIFE (49509).xlsx\n",
      "2018-03-11 00:00:00\n",
      "2018-03-17 00:00:00\n",
      "Sales and Inventory Report - 2018 FW07 - KOCO LIFE (49509).xlsx\n",
      "2018-03-18 00:00:00\n",
      "2018-03-24 00:00:00\n",
      "Sales and Inventory Report - 2018 FW08 - KOCO LIFE (49509).xlsx\n",
      "2018-03-25 00:00:00\n",
      "2018-03-31 00:00:00\n",
      "Sales and Inventory Report - 2018 FW09 - KOCO LIFE (49509).xlsx\n",
      "2018-04-01 00:00:00\n",
      "2018-04-07 00:00:00\n",
      "Sales and Inventory Report - 2018 FW11 - Week ending 4_22_18.xlsx\n",
      "2018-04-15 00:00:00\n",
      "2018-04-21 00:00:00\n",
      "Sales and Inventory Report - 2018 FW12 - KOCO LIFE (49509).xlsx\n",
      "2018-04-22 00:00:00\n",
      "2018-04-28 00:00:00\n",
      "Sales and Inventory Report - 2018 FW13 - KOCO LIFE (49509) (2).xlsx\n",
      "2018-04-29 00:00:00\n",
      "2018-05-05 00:00:00\n",
      "Sales and Inventory Report 4_14_18.xlsx\n",
      "2018-04-08 00:00:00\n",
      "2018-04-14 00:00:00\n",
      "Ulta Sales and Inventory Report - 2018 FW04 - KOCO LIFE (49509).xlsx\n",
      "2018-02-25 00:00:00\n",
      "2018-03-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "rootdir = app_path + filesep + 'kopari' + filesep + 'ulta' + filesep + 'inbox'\n",
    "dfdata = pd.DataFrame()\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xls\") or filepath.endswith(\".xlsx\"):\n",
    "            print(file)\n",
    "            readfiles(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
