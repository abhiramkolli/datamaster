{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "import platform\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import tz\n",
    "import dateutil.parser as dp\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendates(shoptimezone, min_date,max_date,rfreq):\n",
    "    to_zone = tz.gettz(shoptimezone)\n",
    "    dateranges = pd.date_range(start=min_date, end=max_date, freq=rfreq, tz=to_zone)\n",
    "    dateranges = dateranges.union([min_date,max_date])\n",
    "    dfdateranges = pd.DataFrame(dateranges)\n",
    "    dfdateranges.columns=['start_date']\n",
    "    dfdateranges['end_date'] = dfdateranges.start_date.shift(-1)\n",
    "    dfdateranges = dfdateranges[:-1]\n",
    "    dfdateranges['end_date'] = dfdateranges['end_date'] + datetime.timedelta(milliseconds=1)\n",
    "    return dfdateranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcurtimeinshoptz(sz):\n",
    "    from_zone = tz.tzlocal()\n",
    "    to_zone = tz.gettz(sz)\n",
    "    utc = datetime.datetime.now()\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    currentshopdate = utc.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconvtimeinshoptz(sz, t):\n",
    "    to_zone = tz.gettz(sz)\n",
    "    currentshopdate = t.astimezone(to_zone)\n",
    "    return currentshopdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcountandpages(countrurl, headers, cntparams):\n",
    "    totalcnt = requests.get(countrurl, headers = headers, params = cntparams).json()['count']   \n",
    "    nopages = math.ceil(totalcnt/limit) + 1\n",
    "    return totalcnt,nopages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettimezone(shoptimeurl, headers, cntparams):\n",
    "    response = requests.get(shoptimeurl, headers = headers, params = cntparams).json()\n",
    "    df = pd.DataFrame(response['shop'], index=[0])\n",
    "    return df['iana_timezone'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfirstcreationdate(shopifycode, pageurl, headers, params):\n",
    "    params.update({'order' : 'created_at asc'})\n",
    "    response = requests.get(pageurl, headers = headers, params = params).json()\n",
    "    df = pd.DataFrame(response[shopifycode])\n",
    "    min_date = min(df['created_at'])\n",
    "    print(min_date)\n",
    "    return min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshopifydates(rtype, rfreq, min_date, max_date, shopifycode, shoptimezone, countrurl, headers, cntparams, pageurl, params):\n",
    "    if (rtype == runtype[0]) or (rtype == runtype[1] and min_date is None and max_date is None):\n",
    "        currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "        to_zone = tz.gettz(shoptimezone)\n",
    "        min_date_str = getfirstcreationdate(shopifycode, pageurl, headers, params)\n",
    "        min_date = dp.parse(min_date_str)\n",
    "        min_date = min_date.replace(tzinfo=to_zone)\n",
    "        dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "    elif rtype == runtype[1]:\n",
    "        if max_date is None:    \n",
    "            dates = gendates(shoptimezone, min_date, currentshopdate, rfreq)\n",
    "        else:    \n",
    "            dates = gendates(shoptimezone, min_date, max_date, rfreq)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushtojson(dfcontents, dest_file_name):\n",
    "    dfcontents.to_json(dest_file_name,orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestshopifydata(shopifycode, pageurl, params): \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        response = requests_retry_session().get(pageurl, headers = headers, params = params)\n",
    "    except Exception as x:\n",
    "        print('It failed :(', x.__class__.__name__)\n",
    "    else:\n",
    "        print('It eventually worked', response.status_code)\n",
    "    finally:\n",
    "        t1 = time.time()\n",
    "        print('Took', t1 - t0, 'seconds')\n",
    "    #response = requests.get(pageurl, headers = headers, params = params)\n",
    "    df = pd.DataFrame(response.json()[shopifycode])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_details(client_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_details\n",
    "        WHERE client_name = @client_name\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_name', 'STRING', client_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvendor_details(vendor_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.vendor_details\n",
    "        WHERE vendor_name = @vendor_name\n",
    "        ORDER BY vendor_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('vendor_name', 'STRING', vendor_name)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclient_shopify_entitilements(client_id):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        select * from sarasdata.client_shopify_entitilements\n",
    "        WHERE client_id = @client_id\n",
    "        ORDER BY client_id DESC;\n",
    "        \"\"\"\n",
    "    query_params = [\n",
    "        bigquery.ScalarQueryParameter('client_id', 'INTEGER', client_id)\n",
    "    ]\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlastupdateddate(dataset_name, table_name):\n",
    "    client = bigquery.Client()\n",
    "    query = \"select max(updated_at) max_updated_dt from \" + dataset_id + \".\" + table_name + \";\"\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "\n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteexistingrows(dataset_name, table_name, ids):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    query = \"delete from \" + dataset_id + \".\" + table_name + \" where id in UNNEST(@ids);\"\n",
    "    query_params = [\n",
    "        bigquery.ArrayQueryParameter('ids', 'INTEGER', ids)\n",
    "    ]\n",
    "\n",
    "    print(query)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.use_legacy_sql = False\n",
    "    job_config.query_parameters = query_params\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "    query_job.result()  # Wait for job to complete\n",
    "    \n",
    "    # Print the results.\n",
    "    destination_table_ref = query_job.destination\n",
    "    table = client.get_table(destination_table_ref)\n",
    "    table_data = None\n",
    "    for row in client.list_rows(table):\n",
    "        table_data = row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createloadtracker(dataset_id,table_name,file_names,date_from,date_to):\n",
    "    load_id = uuid.uuid4()\n",
    "    dfnew = pd.DataFrame(columns=['load_id','dataset_id','table_name','file_names',\n",
    "    'date_from','date_to','loaded_to_bigquery','bigquery_load_date','creation_date',\n",
    "    'update_date','load_script_version','load_script_file_name'])\n",
    "    row = dict()\n",
    "    row['load_id'] = load_id\n",
    "    row['dataset_id'] = dataset_id\n",
    "    row['table_name'] = table_name\n",
    "    row['file_names'] = file_names\n",
    "    row['date_from'] = date_from.replace(tzinfo=None)\n",
    "    row['date_to'] = date_to.replace(tzinfo=None)\n",
    "    row['loaded_to_bigquery'] = 0\n",
    "    row['bigquery_load_date'] = None\n",
    "    row['creation_date'] = datetime.datetime.now()\n",
    "    row['update_date'] = datetime.datetime.now()\n",
    "    row['load_script_version'] = 'v1'\n",
    "    row['load_script_file_name'] = 'shopifyextract.py'\n",
    "    row_s = pd.Series(row)    \n",
    "    print(row_s)\n",
    "    dfnew = dfnew.append(row_s,ignore_index=True)\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateloadtracker(filename, dataset_id, file_names, table_name, date_from,date_to, delimitertype, loadtype, skipheader):\n",
    "    delimitertype = 'NEWLINE_DELIMITED_CSV'\n",
    "    loadtype = 'WRITE_APPEND'\n",
    "    loadtracker = createloadtracker(dataset_id,table_name,file_names,date_from,date_to)\n",
    "    loadtracker.to_csv(filename, index=False)\n",
    "    #loadfiletobigquery(filename, dataset_id, 'load_tracker', delimitertype, loadtype, skipheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocalfiletogooglestorage(batfile, source_file_name, dest_file_name):\n",
    "    pass_arg=[]\n",
    "    pass_arg.append(batfile)\n",
    "    pass_arg.append(source_file_name)\n",
    "    pass_arg.append(dest_file_name)\n",
    "    p = Popen(pass_arg, stdout=PIPE, stderr=PIPE)\n",
    "    output, errors = p.communicate()\n",
    "    p.wait() # wait for process to terminate\n",
    "    print(output)\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiletobigquery(file_name, dataset_id, table_name, delimitertype, loadtype, skipheader):\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    table_ref = client.dataset(dataset_id).table(table_name)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    if skipheader is not None:\n",
    "        job_config.skip_leading_rows = skipheader\n",
    "    job_config.source_format = delimitertype\n",
    "    #if delimitertype == bigquery.SourceFormat.CSV:\n",
    "        #job_config.autodetect = True\n",
    "    job_config.write_disposition = loadtype\n",
    "    \n",
    "    print(dataset_id)\n",
    "    print(file_name)\n",
    "    print(table_ref)\n",
    "    with open(file_name, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            file_name,\n",
    "            table_ref,\n",
    "            location='US',  # Must match the destination dataset location.\n",
    "            job_config=job_config)  # API request\n",
    "\n",
    "    assert load_job.job_type == 'load'\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "\n",
    "    assert load_job.state == 'DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_path = os.getcwd()\n",
    "gspath = 'gs://sarasmaster'\n",
    "os.chdir(os.getcwd())\n",
    "filesep = '\\\\' if platform.system() == 'Windows' else '/'\n",
    "gssep = '/'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"creds\" + filesep + \"sarasmaster-524142bf5547.json\"\n",
    "gcspath = 'C:\\\\Users\\\\kabhi\\\\AppData\\\\Local\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + gcspath\n",
    "batfile = app_path + filesep + 'movetogcs.bat' if platform.system() == 'Windows' else app_path + filesep + 'movetogcs.sh'\n",
    "delimitertype = 'NEWLINE_DELIMITED_JSON'\n",
    "loadtype = 'WRITE_APPEND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\logs\\shopify_20180506.log\n"
     ]
    }
   ],
   "source": [
    "client_details = getclient_details('Kopari Beauty')\n",
    "client_shopify_entitilements = getclient_shopify_entitilements(client_details.client_id)\n",
    "shopifyurl= client_shopify_entitilements.shop_url\n",
    "cloud_storage_dir = client_shopify_entitilements.cloud_storage_dir\n",
    "access_token = client_shopify_entitilements.access_token\n",
    "project_id = client_details.project_id\n",
    "dataset_id = client_shopify_entitilements.dataset_id\n",
    "pageno = 1\n",
    "limit = 250\n",
    "\n",
    "filepath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'shopify'\n",
    "gcspath = gspath + gssep + client_shopify_entitilements.cloud_storage_dir + gssep + 'shopify'\n",
    "logpath = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'logs'\n",
    "loadtrackerfile = app_path + filesep + client_shopify_entitilements.cloud_storage_dir + filesep + 'loadtracker.csv'\n",
    "loadtrackertable = 'load_tracking'\n",
    "hdlr = logging.FileHandler(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "print(logpath + filesep + 'shopify_' + datetime.datetime.now().strftime('%Y%m%d') + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Start Shopify Load Process')\n",
    "\n",
    "status = 'any'\n",
    "runtype = ['full','incremental']\n",
    "headers = {'content-type' : 'application/json', 'X-Shopify-Access-Token' : access_token}\n",
    "urlparams = {'limit': limit, 'status' : status}\n",
    "cnturlparams = {'status' : status}\n",
    "dot = '.'\n",
    "skipheader = None\n",
    "shopurl = shopifyurl + '/admin/shop.json'\n",
    "shopifycodes = {\n",
    "    'shopifycodes': ['orders', 'customers', 'products'],\n",
    "    'pageurl': [shopifyurl + '/admin/orders.json', shopifyurl + '/admin/customers.json', shopifyurl + '/admin/products.json'],\n",
    "    'countrurl': [shopifyurl + '/admin/orders/count.json', shopifyurl + '/admin/customers/count.json', shopifyurl + '/admin/products/count.json'],\n",
    "    'dest_file_name': [filepath + filesep + 'orders' + filesep + 'inbox' + filesep + 'orders', filepath + filesep + 'customers' + filesep + 'inbox' + filesep + 'customers', filepath + filesep + 'inbox' + filesep + 'products' + filesep + 'inbox' + filesep + 'products'],\n",
    "    'gs_file_path': [gcspath + gssep + 'orders' + gssep + 'inbox', gcspath + gssep + 'customers' + gssep + 'inbox', gcspath + gssep + 'products' + gssep + 'inbox'],\n",
    "    'gs_file_name': [gcspath + gssep + 'orders' + gssep + 'orders', gcspath + gssep + 'customers' + gssep + 'customers', gcspath + gssep + 'products' + gssep + 'products'],\n",
    "    'dest_table_name': ['shopify_orders', 'shopify_customers', 'shopify_products'],\n",
    "    'dest_file_type': ['json', 'json', 'json']\n",
    "}\n",
    "\n",
    "dfshopifycodes = pd.DataFrame(shopifycodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name:orders\n",
      "Total Count:287\n",
      "Total Pages:3\n",
      "Start Date:20180429115246\n",
      "End Date:20180430115246\n",
      "It eventually worked 200\n",
      "Took 0.3761787414550781 seconds\n",
      "It eventually worked 200\n",
      "Took 0.268049955368042 seconds\n",
      "Number of ids to be checked for delete:287\n",
      "Table Name:orders\n",
      "Total Count:359\n",
      "Total Pages:3\n",
      "Start Date:20180430115246\n",
      "End Date:20180501115246\n",
      "It eventually worked 200\n",
      "Took 0.35648345947265625 seconds\n",
      "It eventually worked 200\n",
      "Took 0.27826952934265137 seconds\n",
      "Number of ids to be checked for delete:359\n",
      "Table Name:orders\n",
      "Total Count:477\n",
      "Total Pages:3\n",
      "Start Date:20180501115246\n",
      "End Date:20180502115246\n",
      "It eventually worked 200\n",
      "Took 0.3555562496185303 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3264789581298828 seconds\n",
      "Number of ids to be checked for delete:477\n",
      "Table Name:orders\n",
      "Total Count:943\n",
      "Total Pages:5\n",
      "Start Date:20180502115246\n",
      "End Date:20180503115246\n",
      "It eventually worked 200\n",
      "Took 0.3597080707550049 seconds\n",
      "It eventually worked 200\n",
      "Took 0.5485517978668213 seconds\n",
      "It eventually worked 200\n",
      "Took 0.5708742141723633 seconds\n",
      "It eventually worked 200\n",
      "Took 0.37107205390930176 seconds\n",
      "Number of ids to be checked for delete:943\n",
      "Table Name:orders\n",
      "Total Count:1283\n",
      "Total Pages:7\n",
      "Start Date:20180503115246\n",
      "End Date:20180504115246\n",
      "It eventually worked 200\n",
      "Took 0.34885334968566895 seconds\n",
      "It eventually worked 200\n",
      "Took 0.4484734535217285 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3847382068634033 seconds\n",
      "It eventually worked 200\n",
      "Took 0.39466333389282227 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3640170097351074 seconds\n",
      "It eventually worked 200\n",
      "Took 0.2965352535247803 seconds\n",
      "Number of ids to be checked for delete:1283\n",
      "Table Name:orders\n",
      "Total Count:1413\n",
      "Total Pages:7\n",
      "Start Date:20180504115246\n",
      "End Date:20180505115246\n",
      "It eventually worked 200\n",
      "Took 0.40746641159057617 seconds\n",
      "It eventually worked 200\n",
      "Took 0.37415480613708496 seconds\n",
      "It eventually worked 200\n",
      "Took 0.33809900283813477 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3866102695465088 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3728213310241699 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3210444450378418 seconds\n",
      "Number of ids to be checked for delete:1413\n",
      "Table Name:orders\n",
      "Total Count:658\n",
      "Total Pages:4\n",
      "Start Date:20180505115246\n",
      "End Date:20180506115246\n",
      "It eventually worked 200\n",
      "Took 0.3339090347290039 seconds\n",
      "It eventually worked 200\n",
      "Took 0.3802609443664551 seconds\n",
      "It eventually worked 200\n",
      "Took 0.28061485290527344 seconds\n",
      "Number of ids to be checked for delete:658\n",
      "Table Name:orders\n",
      "Total Count:117\n",
      "Total Pages:2\n",
      "Start Date:20180506115246\n",
      "End Date:20180506210119\n",
      "It eventually worked 200\n",
      "Took 0.4227714538574219 seconds\n",
      "Number of ids to be checked for delete:117\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180429.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180429.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180429.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  2.1 MiB]                                                \\r-\\r- [0 files][  2.1 MiB/  2.1 MiB]                                                \\r- [1 files][  2.1 MiB/  2.1 MiB]                                                \\r\\\\\\r\\r\\nOperation completed over 1 objects/2.1 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180430.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180430.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180430.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  2.8 MiB]                                                \\r-\\r- [0 files][  2.3 MiB/  2.8 MiB]                                                \\r\\\\\\r\\\\ [1 files][  2.8 MiB/  2.8 MiB]                                                \\r\\r\\nOperation completed over 1 objects/2.8 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180501.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180501.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180501.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  3.4 MiB]                                                \\r-\\r- [0 files][  2.3 MiB/  3.4 MiB]                                                \\r\\\\\\r\\\\ [1 files][  3.4 MiB/  3.4 MiB]                                                \\r\\r\\nOperation completed over 1 objects/3.4 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180502.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180502.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180502.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  6.8 MiB]                                                \\r-\\r- [0 files][  2.1 MiB/  6.8 MiB]                                                \\r\\\\\\r|\\r| [0 files][  4.4 MiB/  6.8 MiB]                                                \\r/\\r/ [0 files][  6.7 MiB/  6.8 MiB]                                                \\r/ [1 files][  6.8 MiB/  6.8 MiB]                                                \\r-\\r\\r\\nOperation completed over 1 objects/6.8 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180503.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180503.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180503.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  9.4 MiB]                                                \\r-\\r- [0 files][  2.3 MiB/  9.4 MiB]                                                \\r\\\\\\r|\\r| [0 files][  4.6 MiB/  9.4 MiB]                                                \\r/\\r/ [0 files][  7.0 MiB/  9.4 MiB]                                                \\r-\\r\\\\\\r\\\\ [0 files][  9.3 MiB/  9.4 MiB]                                                \\r\\\\ [1 files][  9.4 MiB/  9.4 MiB]                                                \\r\\r\\nOperation completed over 1 objects/9.4 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180504.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180504.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180504.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  9.8 MiB]                                                \\r-\\r- [0 files][  2.1 MiB/  9.8 MiB]                                                \\r\\\\\\r\\\\ [0 files][  4.4 MiB/  9.8 MiB]                                                \\r|\\r/\\r/ [0 files][  6.7 MiB/  9.8 MiB]                                                \\r-\\r\\\\\\r\\\\ [0 files][  9.0 MiB/  9.8 MiB]                                                \\r\\\\ [1 files][  9.8 MiB/  9.8 MiB]                                                \\r|\\r\\r\\nOperation completed over 1 objects/9.8 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180505.json\n",
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180505.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180505.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/  4.3 MiB]                                                \\r-\\r- [0 files][  2.1 MiB/  4.3 MiB]                                                \\r\\\\\\r\\\\ [0 files][  4.3 MiB/  4.3 MiB]                                                \\r|\\r| [1 files][  4.3 MiB/  4.3 MiB]                                                \\r\\r\\nOperation completed over 1 objects/4.3 MiB.                                      \\r\\n'\n",
      "C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orders\\inbox\\orders_20180506.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\r\\n(datamaster) C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd>gsutil cp C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180506.json gs://sarasmaster/kopari/shopify/orders/inbox \\r\\n'\n",
      "b'Copying file://C:\\\\Users\\\\kabhi\\\\Desktop\\\\smd\\\\kopari\\\\shopify\\\\orders\\\\inbox\\\\orders_20180506.json [Content-Type=application/octet-stream]...\\r\\n/ [0 files][    0.0 B/671.5 KiB]                                                \\r/ [1 files][671.5 KiB/671.5 KiB]                                                \\r\\r\\nOperation completed over 1 objects/671.5 KiB.                                    \\r\\n'\n",
      "bigquery_load_date                                                    None\n",
      "creation_date                                   2018-05-06 23:03:27.107458\n",
      "dataset_id                                                          kopari\n",
      "date_from                                 2018-04-29 11:52:46.001000-07:00\n",
      "date_to                                   2018-05-06 21:01:19.857214-07:00\n",
      "file_names               C:\\Users\\kabhi\\Desktop\\smd\\kopari\\shopify\\orde...\n",
      "load_id                               540956a3-4e7f-471e-90e4-a616694287e5\n",
      "load_script_file_name                                    shopifyextract.py\n",
      "load_script_version                                                     v1\n",
      "loaded_to_bigquery                                                       0\n",
      "table_name                                                  shopify_orders\n",
      "update_date                                     2018-05-06 23:03:27.107458\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'tzfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-7ea0d22a05ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mloadlocalfiletogooglestorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gs_file_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalfilelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mupdateloadtracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloadtrackerfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dest_table_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimitertype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloadtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-236-3a2f1ef54ec7>\u001b[0m in \u001b[0;36mupdateloadtracker\u001b[1;34m(filename, dataset_id, file_names, table_name, date_from, date_to, delimitertype, loadtype, skipheader)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdelimitertype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NEWLINE_DELIMITED_CSV'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mloadtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'WRITE_APPEND'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloadtracker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateloadtracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdate_from\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdate_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mloadtracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#loadfiletobigquery(filename, dataset_id, 'load_tracker', delimitertype, loadtype, skipheader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-235-9dc1f5531626>\u001b[0m in \u001b[0;36mcreateloadtracker\u001b[1;34m(dataset_id, table_name, file_names, date_from, date_to)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mrow_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdfnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m   5178\u001b[0m                               \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m                               columns=combined_columns)\n\u001b[1;32m-> 5180\u001b[1;33m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(self, datetime, numeric, timedelta, coerce, copy)\u001b[0m\n\u001b[0;32m   4064\u001b[0m             self._data.convert(datetime=datetime, numeric=numeric,\n\u001b[0;32m   4065\u001b[0m                                \u001b[0mtimedelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4066\u001b[1;33m                                copy=copy)).__finalize__(self)\n\u001b[0m\u001b[0;32m   4067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[1;31m# TODO: Remove in 0.18 or 2017, which ever is sooner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'convert'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3328\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3329\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3330\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mby_item\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_single_block\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2135\u001b[1;33m             \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_and_operate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36msplit_and_operate\u001b[1;34m(self, mask, f, inplace)\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[1;31m# need a new block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                 \u001b[0mnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                 \u001b[0mnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(m, v, i)\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2125\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2126\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\datamaster\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36msoft_convert_objects\u001b[1;34m(values, datetime, numeric, timedelta, coerce, copy)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[1;31m# Soft conversions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_datetime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtimedelta\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'tzfile'"
     ]
    }
   ],
   "source": [
    "shoptimezone = gettimezone(shopurl, headers, cnturlparams)\n",
    "currentshopdate = getcurtimeinshoptz(shoptimezone)\n",
    "runmode = runtype[1]\n",
    "runfreq = 'D'\n",
    "for row_index,row in dfshopifycodes.iterrows():\n",
    "    lastshopdate = getlastupdateddate(dataset_id, row['dest_table_name']).max_updated_dt\n",
    "    if lastshopdate is not None:\n",
    "        lastshopdate = getconvtimeinshoptz(shoptimezone, lastshopdate)\n",
    "        lastshopdate = lastshopdate + datetime.timedelta(milliseconds=1)\n",
    "        start_date = lastshopdate\n",
    "        end_date = currentshopdate\n",
    "    else:\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "        \n",
    "    dates = getshopifydates(runmode,runfreq,start_date,end_date,row['shopifycodes'], shoptimezone, row['countrurl'], headers, cnturlparams, row['pageurl'], urlparams)\n",
    "    ids = []\n",
    "    localfilelist = []\n",
    "    gcsfilelist = []\n",
    "    for dates_index, dates_row in dates.iterrows():\n",
    "        cntparams = cnturlparams\n",
    "        cntparams.update({'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "        totalcnt,nopages = getcountandpages(row['countrurl'], headers, cntparams)\n",
    "        print(\"Table Name:\" + row['shopifycodes'])\n",
    "        print(\"Total Count:\" + str(totalcnt))\n",
    "        print(\"Total Pages:\" + str(nopages))\n",
    "        print(\"Start Date:\" + dates_row['start_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        print(\"End Date:\" + dates_row['end_date'].strftime('%Y%m%d%H%M%S'))\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(1,nopages):\n",
    "            params = urlparams\n",
    "            params.update({'page': i,'updated_at_min' : dates_row['start_date'],'updated_at_max' : dates_row['end_date']})\n",
    "            df1 = requestshopifydata(row['shopifycodes'], row['pageurl'], params)\n",
    "            df=df.append(df1,ignore_index=True)\n",
    "            time.sleep(1)\n",
    "        ids.extend(df['id'].tolist()) if df.shape[0] > 0 else ids\n",
    "        localfilename = row['dest_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "        gcsfilename = row['gs_file_name'] + '_' + dates_row['start_date'].strftime('%Y%m%d') + dot + row['dest_file_type']\n",
    "        localfilelist.append(localfilename)\n",
    "        gcsfilelist.append(gcsfilename)\n",
    "        pushtojson(df, localfilename)\n",
    "        print(\"Number of ids to be checked for delete:\" + str(len(df['id'].tolist())))\n",
    "        #deleteexistingrows(dataset_id, row['dest_table_name'], df['id'].tolist())        \n",
    "\n",
    "    \n",
    "    for localfilename in localfilelist:\n",
    "        print(localfilename)\n",
    "        #loadlocalfiletogooglestorage(batfile, localfilename, row['gs_file_path'])\n",
    "    filenames = ','.join(localfilelist)\n",
    "    updateloadtracker(loadtrackerfile, dataset_id, filenames, row['dest_table_name'], start_date, end_date, delimitertype, loadtype, skipheader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
